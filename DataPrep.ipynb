{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26986790",
   "metadata": {},
   "source": [
    "### Preprocessor\n",
    "#### The preprocessor will take in raw text as input and transform it in the following in order: \n",
    "1. remove punctuation, 2. remove numbers 3. lower case, 4. tokenization, 5. lemmatization, 6. remove extra white space.\n",
    "The preprocessor class when instantiated with the input (only takes one line of code) should return the transformed output as word tokenized and lemmatized string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1a490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "909792e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "numbers = [str(n) for n in range(0,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbb38ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes in raw text and retuns tokenized and lemmatized list of words after applying a number preprocessing\n",
    "# steps mentioned in paper: \n",
    "def preprocess(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocessing with the following:\n",
    "    1. remove punctuation, 2. remove numbers 3. lower case, 4. tokenization, 5. lemmatization, 6. remove extra white space.\n",
    "    : return: a list of tokenized and lemmatized words.\n",
    "    \"\"\"\n",
    "    text = ''.join([c for c in text if c not in punctuation]) # remove punctuation\n",
    "    text = ''.join([c for c in text if c not in numbers]) # remove numbers\n",
    "    preprocessed_text = text.lower() # lower case the text.\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_lemmatize(text: str) -> list:\n",
    "    \"\"\"\n",
    "    :Tokenize text into list of words.\n",
    "    :Lemmatize these words to standardise them into their roots.\n",
    "    :Remove any spaces within a stored word lemma for standardisation.\n",
    "    Return: a standardised, lemmatized list of tokenized words. \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized_text = word_tokenize(text) # returns a tokenized list of words from text.\n",
    "    tokenized_text = [ps.stem(word) for word in tokenized_text] # returns a lemmatization of the tokenized text (effectively reducing vocab)\n",
    "    tokenized_text = [word.replace(' ', '') for word in tokenized_text]  \n",
    "    return tokenized_text\n",
    "\n",
    "\n",
    "\n",
    "def create_lookup_tables(words: iter) -> tuple:\n",
    "    \"\"\"\n",
    "    Create lookup words for vocabulary.\n",
    "    :the 'words' argument or parameter: takes in a list of words\n",
    "    Return: Three dictionaries, vocab_int and int_vocab\n",
    "    \"\"\"\n",
    "    word_count = Counter(words)\n",
    "    # sorting the frequency of words from highest to lowest in occurrence.\n",
    "    sorted_word_count = sorted(word_count, key=word_count.get, reverse=True)\n",
    "    # creating dicts that has key-value pairs of word-count and count-word.\n",
    "    vocab_int = {word: (ii+1) for ii, word in enumerate(sorted_word_count)}\n",
    "    int_vocab = {(ii+1): word for ii, word in enumerate(sorted_word_count)}\n",
    "    \n",
    "    # add the unknown word to dict\n",
    "    vocab_int['<unk>'] = 0\n",
    "    int_vocab[0] = '<unk>'\n",
    "    \n",
    "    return word_count, vocab_int, int_vocab\n",
    "\n",
    "\n",
    "def word_to_int(input_text: iter, vocab_to_int: dict) -> iter:\n",
    "    \"\"\"\n",
    "    A function to be used for encoding text to integers for prediction (assigning unknown words).\n",
    "    Return: list of integers representing words in text\n",
    "    \"\"\"\n",
    "    standardised_text = tokenize_lemmatize(input_text)\n",
    "    # Convert words not in lookup to '<unk>'.\n",
    "    for ii, word in enumerate(standardised_text):\n",
    "        if word not in list(vocab_to_int.keys()):\n",
    "            standardised_list[ii] = '<unk>'\n",
    "        # assign the text integer values.\n",
    "    word_ints = [vocab_to_int[word] for word in standardised_text]\n",
    "    \n",
    "    return word_ints\n",
    "\n",
    "def pad_features(reviews_ints: iter, seq_length: int):\n",
    "    ''' \n",
    "    : Take in a list of words encoded as integers, list length parametarised by 'seq_length',\n",
    "    then return them as input numpy array features for the model. \n",
    "    Return: features of review_ints, where each review is padded with 0's \n",
    "    or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
