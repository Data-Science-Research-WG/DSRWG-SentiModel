{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26986790",
   "metadata": {},
   "source": [
    "### Utils for preparing data to feed into model and for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc1a490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "909792e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "numbers = [str(n) for n in range(0,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbb38ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes in raw text and retuns tokenized and lemmatized list of words after applying a number preprocessing\n",
    "# steps mentioned in paper: \n",
    "def preprocess(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocessing with the following:\n",
    "    1. remove punctuation, 2. remove numbers 3. lower case\n",
    "    : return: a list of tokenized and lemmatized words.\n",
    "    \"\"\"\n",
    "    text = ''.join([c for c in text if c not in punctuation]) # remove punctuation\n",
    "    text = ''.join([c for c in text if c not in numbers]) # remove numbers\n",
    "    preprocessed_text = text.lower() # lower case the text.\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_lemmatize(text: str) -> list:\n",
    "    \"\"\"\n",
    "    :Tokenize text into list of words.\n",
    "    :Lemmatize these words to standardise them into their roots.\n",
    "    :Remove any spaces within a stored word lemma for standardisation.\n",
    "    Return: a standardised, lemmatized list of tokenized words. \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized_text = word_tokenize(text) # returns a tokenized list of words from text.\n",
    "    tokenized_text = [ps.stem(word) for word in tokenized_text] # returns a lemmatization of the tokenized text (effectively reducing vocab)\n",
    "    tokenized_text = [word.replace(' ', '') for word in tokenized_text]  \n",
    "    return tokenized_text\n",
    "\n",
    "\n",
    "def subsample(words: iter, threshold=1e-5) -> iter:\n",
    "    \"\"\"Subsampling in order to get rid of most frequent words that add noise to the data.\n",
    "    :words: [iter] a list-like structure of words\"\"\"\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    total_count = len(words)\n",
    "    \n",
    "    freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "    # the probability that a word will be dropped from the paper, \n",
    "    # 'Efficient Estimation of Word representation in vector space':-\n",
    "    \"\"\"https://arxiv.org/pdf/1301.3781.pdf\"\"\"\n",
    "    \n",
    "    p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "    # discard some frequent words, according to the subsampling equation\n",
    "    # create a new list of words for training\n",
    "    train_words = [word for word in words if random.random() < (1 - p_drop[word])]\n",
    "    return train_words\n",
    "\n",
    "\n",
    "def create_lookup_tables(words: iter, subsampling=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Create lookup words for vocabulary.\n",
    "    :the 'words' argument or parameter: takes in a list of words\n",
    "    :subsampling - if true, apply subsampling to the word list to remove some 'noisy' words.\n",
    "    Return: Three dictionaries, vocab_int and int_vocab\n",
    "    \"\"\"        \n",
    "    word_count = Counter(words)\n",
    "    # sorting the frequency of words from highest to lowest in occurrence.\n",
    "    sorted_word_count = sorted(word_count, key=word_count.get, reverse=True)\n",
    "    # creating dicts that has key-value pairs of word-count and count-word.\n",
    "    vocab_int = {word: (ii+2) for ii, word in enumerate(sorted_word_count)} # plus two to index to allow for 'unk' and padding features with 0.\n",
    "    int_vocab = {(ii+2): word for ii, word in enumerate(sorted_word_count)}\n",
    "    \n",
    "    # add the unknown word to dict\n",
    "    vocab_int['<unk>'] = 1\n",
    "    int_vocab[1] = '<unk>'\n",
    "    \n",
    "    return word_count, vocab_int, int_vocab\n",
    "\n",
    "\n",
    "def word_to_int(input_text: iter, vocab_to_int: dict, token_lem=False) -> iter:\n",
    "    \"\"\"\n",
    "    A function to be used for encoding text to integers for prediction (assigning unknown words).\n",
    "    Return: list of integers representing words in text\n",
    "    \"\"\"\n",
    "    if token_lem == True:\n",
    "        standardised_text = tokenize_lemmatize(input_text)\n",
    "    else:\n",
    "        standardised_text = input_text.split()\n",
    "    # Convert words not in lookup to '<unk>'.\n",
    "    \n",
    "    word_ints = []\n",
    "    for ii, word in enumerate(standardised_text):\n",
    "        try:\n",
    "            word_ints.append(vocab_to_int[word])\n",
    "        except KeyError:\n",
    "            word_ints.append(1)\n",
    "        # assign the text integer values.   \n",
    "    return word_ints\n",
    "\n",
    "def pad_features(reviews_ints: iter, seq_length: int):\n",
    "    ''' \n",
    "    : Take in a list of words encoded as integers, list length parametarised by 'seq_length',\n",
    "    then return them as input numpy array features for the model. \n",
    "    Return: features of review_ints, where each review is padded with 0's \n",
    "    or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27153623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(model, new_texts, vocab_to_int, seq_length=40):\n",
    "    \"\"\"\n",
    "    Function that takes in text, preproceses and passes it to the model for forward pass.\n",
    "    Args: \n",
    "     - model to perform the inference\n",
    "     - input text\n",
    "     - word to integer mapping dict\n",
    "     - sequence length the text is padded to\n",
    "    :Returns a score of positive or negative.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # preprocess, tokenize and lemmatize review\n",
    "    new_texts = preprocess(new_texts)\n",
    "    new_texts_ints = word_to_int(new_texts, vocab_to_int, token_lem=True)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    features = np.zeros((seq_length), dtype=int)\n",
    "    if features.shape[0] >= len(new_texts_ints):\n",
    "        features[seq_length-len(new_texts_ints):] = np.array(new_texts_ints)[:seq_length]\n",
    "    else:\n",
    "        features[:] = np.array(new_texts_ints)[:seq_length]\n",
    "    \n",
    "    input_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    # perform a forward pass from the model\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    pred = output.detach().numpy()[0][0]\n",
    "    if pred >= 0.55:\n",
    "        return (\"positive, {:.4f}\".format(2*pred - 1))\n",
    "    elif pred < 0.45: \n",
    "        return (\"negative, {:.4f}\".format(2*pred - 1))\n",
    "    else:\n",
    "        return (\"neutral, {:.4f}\".format(2*pred - 1))\n",
    "    \n",
    "\n",
    "\n",
    "def class_predict(model, new_texts, vocab_to_int, seq_length=40):\n",
    "    \"\"\"\n",
    "    Function that takes in text, preproceses and passes it to the model for forward pass.\n",
    "    Args: \n",
    "     - model to perform the inference\n",
    "     - input text\n",
    "     - word to integer mapping dict\n",
    "     - sequence length the text is padded to\n",
    "    :Returns a score of positive or negative.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # preprocess, tokenize and lemmatize review\n",
    "    new_texts = preprocess(new_texts)\n",
    "    new_texts_ints = word_to_int(new_texts, vocab_to_int, token_lem=True)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    features = np.zeros((seq_length), dtype=int)\n",
    "    if features.shape[0] >= len(new_texts_ints):\n",
    "        features[seq_length-len(new_texts_ints):] = np.array(new_texts_ints)[:seq_length]\n",
    "    else:\n",
    "        features[:] = np.array(new_texts_ints)[:seq_length]\n",
    "    \n",
    "    input_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    # perform a forward pass from the model\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    pred = output.detach().numpy()[0][0]\n",
    "    if pred >= 0.5:\n",
    "        return 1\n",
    "    elif pred < 0.5: \n",
    "        return 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
