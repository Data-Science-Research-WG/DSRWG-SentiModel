{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03cfa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.  Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "%run Utils.ipynb\n",
    "%run models/SentimentCNN_model.ipynb\n",
    "%run TrainTestSentimentCNN.ipynb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a797fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data = pd.read_csv('Data/amazon_review_polarity/train.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2142e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data.columns = ['sentiment', 'title', 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd3cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode(val):\n",
    "    if val == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1092a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data['sentiment'] = reviews_data['sentiment'].apply(lambda x: recode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2e3302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews = reviews_data[reviews_data['sentiment'] == 1].sample(1000000)\n",
    "neg_reviews = reviews_data[reviews_data['sentiment'] == 0].sample(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c127c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data = pd.concat([pos_reviews, neg_reviews], axis=0).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47bc9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data = reviews_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c8fe6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999952, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11fc3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading in tensors for Amazon review sub-dataset.\n",
    "# import torch\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ##### Saved data for 200k records, batch_size=250, embedding_size=300, seq_length=30, vocab_size=321951 \n",
    "# batch_size = 1024\n",
    "# vocab_size = 720628\n",
    "\n",
    "# train_data = TensorDataset(torch.load(\"Data/amazon_review_polarity/Amazon_polarity_subset800k_trainX.pt\"), \n",
    "#                            torch.load(\"Data/amazon_review_polarity/Amazon_polarity_subset800k_trainy.pt\"))\n",
    "# train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# valid_data = TensorDataset(torch.load(\"Data/amazon_review_polarity/Amazon_polarity_subset800k_valX.pt\"),\n",
    "#                            torch.load(\"Data/amazon_review_polarity/Amazon_polarity_subset800k_valy.pt\"))\n",
    "# valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# test_data = TensorDataset(torch.load(\"Data/amazon_review_polarity/Amazon_polarity_subset800k_testX.pt\"), \n",
    "#                           torch.load(\"Data/amazon_review_polarity/Amazon_polarity_subset800k_testy.pt\"))\n",
    "# test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741e615",
   "metadata": {},
   "source": [
    "### Preprocess and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b25c21",
   "metadata": {},
   "source": [
    "#### Initial preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4368b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # get reviews in a list from the pd.Series\n",
    "reviews_list = reviews_data['review'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85ed051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_list = reviews_data['sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9cd5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the reviews in accordance to the preprocess function.\n",
    "preprocessed_reviews_list = [preprocess(review) for review in reviews_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f681f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text and without stemmatizing it to preprocess more\n",
    "tokenized_text = ' '.join(preprocessed_reviews_list).split() # stem the tokenized words and replace any extra white spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943ab58",
   "metadata": {},
   "source": [
    "#### Encoding words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "123057bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO LEMMATIZATION: creating look up table for encoding words to integers, while getting back their frequency or occurrence.\n",
    "word_count, vocab_int, int_vocab = create_lookup_tables(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b062b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('Data/amazon_review_polarity/Amazon_polarity_subset2m_vocab_to_int.json', 'w') as f:\n",
    "    json.dump(vocab_int, f)\n",
    "        \n",
    "with open('Data/amazon_review_polarity/Amazon_polarity_subset2m_int_to_vocab.json', 'w') as fp:\n",
    "    json.dump(int_vocab, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4cb2e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO LEMMATIZATION: numerical encoding\n",
    "reviews_ints = []\n",
    "for review in preprocessed_reviews_list:\n",
    "    reviews_ints.append([vocab_int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8c12045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[79221, 131192, 140902, 698488, 768026, 1228268]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_to_drop = [i for i, ints  in enumerate(reviews_ints) if len(ints) == 0]\n",
    "indices_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19d6ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ints = [review for i,review in enumerate(reviews_ints) if i not in indices_to_drop]\n",
    "senti_list = [senti for i, senti in enumerate(senti_list) if i not in indices_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2b115cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999946\n",
      "1999946\n"
     ]
    }
   ],
   "source": [
    "# senti_list = senti_list[:799744] # make it divisible by batch size 250\n",
    "# reviews_ints = reviews_ints[:799744] # make it divisible by batch size 250\n",
    "print(len(senti_list))\n",
    "print(len(reviews_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275fc301",
   "metadata": {},
   "source": [
    "### Padding Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b0df7",
   "metadata": {},
   "source": [
    "##### Setting up hyperparameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc4740b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.39462765494669\n",
      "41.90018748302791\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "review_lens = [len(review) for review in reviews_ints]\n",
    "review_len_mean = np.array(review_lens).mean()\n",
    "review_len_std = np.array(review_lens).std()\n",
    "print(review_len_mean)\n",
    "print(review_len_std)\n",
    "print(max(review_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7124d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "try:\n",
    "    vocab_size = len(vocab_int) + 1 # for the 0 padding + our word tokens\n",
    "except:\n",
    "    vocab_size = vocab_size\n",
    "\n",
    "embedding_size = 300\n",
    "# seq_length = int(review_len_mean + 2*(review_len_std))\n",
    "seq_length = 250\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5276bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pad_features(reviews_ints, seq_length)\n",
    "sentiments = np.array(senti_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0ec4482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999946,)\n",
      "(1999946, 250)\n"
     ]
    }
   ],
   "source": [
    "print(sentiments.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "538d8580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(1899948, 250) \n",
      "Validation set: \t(49999, 250) \n",
      "Test set: \t\t(49999, 250)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.95\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = sentiments[:split_idx], sentiments[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape)\n",
    ",\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8876c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 4096\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c882096",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch.from_numpy(train_x), \"Data/amazon_review_polarity/Amazon_polarity_subset2m_trainX.pt\")\n",
    "torch.save(torch.from_numpy(train_y), \"Data/amazon_review_polarity/Amazon_polarity_subset2m_trainy.pt\")\n",
    "torch.save(torch.from_numpy(val_x), \"Data/amazon_review_polarity/Amazon_polarity_subset2m_valX.pt\")\n",
    "torch.save(torch.from_numpy(val_y), \"Data/amazon_review_polarity/Amazon_polarity_subset2m_valy.pt\")\n",
    "torch.save(torch.from_numpy(test_x), \"Data/amazon_review_polarity/Amazon_polarity_subset2m_testX.pt\")\n",
    "torch.save(torch.from_numpy(test_y), \"Data/amazon_review_polarity/Amazon_polarity_subset2m_testy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb4185df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3f7209f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 250])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b5581c",
   "metadata": {},
   "source": [
    "#### Setting Hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00ac2498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed = nn.Embedding(vocab_size, embedding_size)\n",
    "# e = embed(sample[0]).reshape(batch_size, seq_length, embedding_size)\n",
    "\n",
    "# conv = nn.Conv1d(seq_length, 256, 3)\n",
    "# x = F.relu(conv(e))\n",
    "# print(\"init conv: \", conv)\n",
    "\n",
    "# conv1 = nn.Conv1d(256, 128, 3)\n",
    "# x = F.relu(conv1(x))\n",
    "# print(\"conv1: \", x.shape)\n",
    "\n",
    "# pool = nn.MaxPool1d(3, 3)\n",
    "# x = pool1(F.relu(x))\n",
    "# print(\"pool: \", x.shape)\n",
    "\n",
    "# conv2 = nn.Conv1d(128, 64, 3)\n",
    "# x = F.relu(conv2(x))\n",
    "# print(\"conv2: \",x.shape)\n",
    "\n",
    "# conv3 = nn.Conv1d(64, 32, 3)\n",
    "# x = conv3(x)\n",
    "# print(\"conv3: \",x.shape)\n",
    "\n",
    "# pool1 = nn.MaxPool1d(3,3)\n",
    "# x = pool1(F.relu(x))\n",
    "# print(\"pool1: \",x.shape)\n",
    "\n",
    "# conv4 = nn.Conv1d(32, 16, 3)\n",
    "# x = conv4(x)\n",
    "# print(\"conv4: \",x.shape)\n",
    "\n",
    "# conv5 = nn.Conv1d(16, 8, 3)\n",
    "# x = conv5(x)\n",
    "# print(\"conv5: \", x.shape)\n",
    "\n",
    "# avgpool = nn.AvgPool1d(27)\n",
    "# x = avgpool(x)\n",
    "# print(\"avgpool: \",x.shape)\n",
    "\n",
    "# x = x.view(batch_size, -1)\n",
    "# print(\"flattened: \", x.shape)\n",
    "\n",
    "# fc = nn.Linear(8, 1)\n",
    "# x = fc(x)\n",
    "# print(\"final output shape\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d4cc9",
   "metadata": {},
   "source": [
    "#### Instantiate model with parameters (currently only working with seq_length = 40, batchsize=50 and output_size=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6f2e20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentCNN(\n",
      "  (embedding): Embedding(1371176, 300)\n",
      "  (conv1): Conv1d(250, 64, kernel_size=(3,), stride=(1,))\n",
      "  (conv2): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
      "  (pool1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(32, 16, kernel_size=(3,), stride=(1,))\n",
      "  (conv4): Conv1d(16, 8, kernel_size=(3,), stride=(1,))\n",
      "  (avgpool): AvgPool1d(kernel_size=(94,), stride=(94,), padding=(0,))\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SentimentCNN(vocab_size, output_size, embedding_size, batch_size, seq_length)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26723d",
   "metadata": {},
   "source": [
    "### Train or Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351387a8",
   "metadata": {},
   "source": [
    "#### Specifying learning_rate, Loss functions and Optimizer for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba37ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001 # learning rate to be used for the optimizer.\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a6f05",
   "metadata": {},
   "source": [
    "#### Import the train model function from TrainTestSentimentCNN ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fc8529",
   "metadata": {},
   "source": [
    "##### TRAIN MODEL: UNCOMMENT CELL BELOW TO RUN TRAINING PROCEDURE. \n",
    "Current model trains for 5 epochs and saves the model params whenever validation loss hits a min value after an epoch of training. That pre-trained model can then be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7155ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fail index:  464\n",
      "Epoch 1 completed in: 102.866  minutes\n",
      "valid fail index:  13\n",
      "Epoch: 1 \tTraining Loss: 0.476961 \tValidation Loss: 0.318019\n",
      "Validation loss decreased (inf --> 0.318019).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [1:43:20<6:53:21, 6200.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fail index:  464\n",
      "Epoch 2 completed in: 101.846  minutes\n",
      "valid fail index:  13\n",
      "Epoch: 2 \tTraining Loss: 0.319489 \tValidation Loss: 0.289868\n",
      "Validation loss decreased (0.318019 --> 0.289868).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [3:25:39<5:08:13, 6164.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fail index:  464\n",
      "Epoch 3 completed in: 106.063  minutes\n",
      "valid fail index:  13\n",
      "Epoch: 3 \tTraining Loss: 0.292132 \tValidation Loss: 0.281954\n",
      "Validation loss decreased (0.289868 --> 0.281954).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [5:12:11<3:28:56, 6268.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fail index:  464\n",
      "Epoch 4 completed in: 102.120  minutes\n",
      "valid fail index:  13\n",
      "Epoch: 4 \tTraining Loss: 0.278135 \tValidation Loss: 0.278976\n",
      "Validation loss decreased (0.281954 --> 0.278976).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [6:54:46<1:43:43, 6223.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fail index:  464\n",
      "Epoch 5 completed in: 105.971  minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [8:41:08<00:00, 6253.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid fail index:  13\n",
      "Epoch: 5 \tTraining Loss: 0.266269 \tValidation Loss: 0.288501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_sentimentCNN(model, train_loader, valid_loader, criterion, optimizer, lr, save_model_as='Sentiment_CNN_subset2m_amazon_pol', n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1b93aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model with the trained parameters/weight that performed best in validation.\n",
    "model.load_state_dict(torch.load('Sentiment_CNN_subset2m_amazon_pol.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76452dd1",
   "metadata": {},
   "source": [
    "##### TEST MODEL: UNCOMMENT CELL BELOW TO RUN TEST PROCEDURE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3e5d630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "print(seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3312cdab",
   "metadata": {},
   "source": [
    "#### Load amazon test data to test against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7b79643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Test Accuracy: 0.8839', 'Precision: 0.8841', 'Recall: 0.8839', 'F1: 0.8838')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentimentCNN(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c1f5f",
   "metadata": {},
   "source": [
    "### Inference \n",
    "Looking at the model's performance against any input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf698e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, new_texts, vocab_to_int, batch_size, seq_length=40):\n",
    "    \"\"\"\n",
    "    Function that takes in text, preproceses and passes it to the model for forward pass.\n",
    "    Args: \n",
    "     - model to perform the inference\n",
    "     - input text\n",
    "     - word to integer mapping dict\n",
    "     - sequence length the text is padded to\n",
    "    :Returns a score of positive or negative.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # preprocess, tokenize and lemmatize review\n",
    "    new_texts = preprocess(new_texts)\n",
    "    new_texts_ints = word_to_int(new_texts, vocab_to_int, token_lem=True)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    features = np.zeros((seq_length), dtype=int)\n",
    "    if features.shape[0] >= len(new_texts_ints):\n",
    "        features[seq_length-len(new_texts_ints):] = np.array(new_texts_ints)[:seq_length]\n",
    "    else:\n",
    "        features[::] = np.array(new_texts_ints)[:seq_length]\n",
    "    \n",
    "    # make the batch size of the features presented to be the size the model was trained on. Default = 50.\n",
    "    model_input = np.zeros((batch_size, seq_length), dtype=int)\n",
    "    model_input[0, :] = features\n",
    "    input_tensor = torch.from_numpy(model_input)\n",
    "    \n",
    "    # perform a forward pass from the model\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    pred = output.detach().numpy()[0][0]\n",
    "    \n",
    "    if pred >= 0.55:\n",
    "        return (\"positive\", 2*pred - 1)\n",
    "    elif pred <= 0.45: \n",
    "        return (\"negative\", 2*pred - 1)\n",
    "    else:\n",
    "        return (\"unsure/neutral\", 2*pred - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efca1f6",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "##### Apply the prediction function on a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "921b7844",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [\n",
    "\"poor quality signal given by the device package did not arrive I am not happy with this\"\n",
    ",'I actually liked that part of the feature It was surprising in a good way and I will surely go back again'\n",
    ",\"I want to speak to a person\"\n",
    ",\"Bad service\"\n",
    ",'Broken appliance'\n",
    ",'Hey, I got a broken item'\n",
    ",'Hi, an item is missing from my order'\n",
    ",'Item came broken'\n",
    ",'My item arrived damaged'\n",
    ",'My product arrived broken'\n",
    ", \"my delivery is late I want to cancel my order\"\n",
    ",'Poor service!'\n",
    ",'The delivery was terrible'\n",
    ",'can I make a complaint to an agent?'\n",
    ",\"I am very happy with the product. It's great!\"\n",
    ",'damaged item'\n",
    ",'no forget it'\n",
    ",'I am happy'\n",
    ",'Cancel my order'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa9a3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('Data/amazon_review_polarity/Amazon_polarity_subset2m_vocab_to_int.json', 'r') as vi:\n",
    "    vocab_to_int = json.load(vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c509aa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poor quality signal given by the device package did not arrive I am not happy with this \n",
      " Sentiment:  ('negative', -0.9950658977031708) \n",
      "\n",
      "I actually liked that part of the feature It was surprising in a good way and I will surely go back again \n",
      " Sentiment:  ('positive', 0.6080207824707031) \n",
      "\n",
      "I want to speak to a person \n",
      " Sentiment:  ('unsure/neutral', 0.02608168125152588) \n",
      "\n",
      "Bad service \n",
      " Sentiment:  ('negative', -0.21603453159332275) \n",
      "\n",
      "Broken appliance \n",
      " Sentiment:  ('negative', -0.16190427541732788) \n",
      "\n",
      "Hey, I got a broken item \n",
      " Sentiment:  ('negative', -0.3867241144180298) \n",
      "\n",
      "Hi, an item is missing from my order \n",
      " Sentiment:  ('positive', 0.2554647922515869) \n",
      "\n",
      "Item came broken \n",
      " Sentiment:  ('negative', -0.27792930603027344) \n",
      "\n",
      "My item arrived damaged \n",
      " Sentiment:  ('positive', 0.18851399421691895) \n",
      "\n",
      "My product arrived broken \n",
      " Sentiment:  ('negative', -0.17975521087646484) \n",
      "\n",
      "my delivery is late I want to cancel my order \n",
      " Sentiment:  ('negative', -0.3208693861961365) \n",
      "\n",
      "Poor service! \n",
      " Sentiment:  ('negative', -0.673427402973175) \n",
      "\n",
      "The delivery was terrible \n",
      " Sentiment:  ('unsure/neutral', 0.0150374174118042) \n",
      "\n",
      "can I make a complaint to an agent? \n",
      " Sentiment:  ('positive', 0.8990042209625244) \n",
      "\n",
      "I am very happy with the product. It's great! \n",
      " Sentiment:  ('positive', 0.5643197298049927) \n",
      "\n",
      "damaged item \n",
      " Sentiment:  ('positive', 0.11859536170959473) \n",
      "\n",
      "no forget it \n",
      " Sentiment:  ('unsure/neutral', -0.08888602256774902) \n",
      "\n",
      "I am happy \n",
      " Sentiment:  ('positive', 0.10091865062713623) \n",
      "\n",
      "Cancel my order \n",
      " Sentiment:  ('unsure/neutral', 0.011101126670837402) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in test_list:\n",
    "    print(t, \"\\n\", \"Sentiment: \", predict(model, t, vocab_int,batch_size=4096, seq_length=seq_length), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f7afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
